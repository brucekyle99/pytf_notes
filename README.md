# pytf_notes
take_notes

#1
tf.argmax函数说明   https://blog.csdn.net/kdongyi/article/details/82390394

#2
BERT 和  XLNET
BERT 这样基于去噪自编码器的预训练模型可以很好地建模双向语境信息，性能优于基于自回归语言模型的预训练方法。然而，由于需要 mask 一部分输入，BERT 忽略了被 mask 位置之间的依赖关系，因此出现预训练和微调效果的差异（pretrain-finetune discrepancy）。

基于这些优缺点，该研究提出了一种泛化的自回归预训练模型 XLNet。XLNet 可以：1）通过最大化所有可能的因式分解顺序的对数似然，学习双向语境信息；2）用自回归本身的特点克服 BERT 的缺点。此外，XLNet 还融合了当前最优自回归模型 Transformer-XL 的思路。作者从自回归（autoregressive）和自编码（autoencoding）两大范式分析了当前的预训练语言模型，并发现它们虽然各自都有优势，但也都有难以解决的困难。为此，研究者提出 XLNet，并希望结合大阵营的优秀属性。

AR 语言建模旨在利用自回归模型估计文本语料库的概率分布。由于 AR 语言模型仅被训练用于编码单向语境（前向或后向），因而在深度双向语境建模中效果不佳。而下游语言理解任务通常需要双向语境信息。这导致 AR 语言建模无法实现有效预训练。

相反，基于 AE 的预训练模型不会进行明确的密度估计，而是从残缺的输入中重建原始数据。一个著名的例子就是 BERT。给出输入 token 序列，BERT 将一部分 token 替换为特殊符号 [MASK]，随后训练模型从残缺版本恢复原始的 token。由于密度估计不是目标的一部分，BERT 允许使用双向语境进行重建。

但是，模型微调时的真实数据缺少 BERT 在预训练期间使用的 [MASK] 等人工符号，这导致预训练和微调之间存在差异。此外，由于输入中预测的 token 是被 mask 的，因此 BERT 无法像自回归语言建模那样使用乘积法则（product rule）对联合概率进行建模。

换言之，给定未 mask 的 token，BERT 假设预测的 token 之间彼此独立，这被过度简化为自然语言中普遍存在的高阶、长期依赖关系。

这篇新研究提出了一种泛化自回归方法 XLNet，既集合了 AR 和 AE 方法的优势，又避免了二者的缺陷。首先，XLNet 不使用传统 AR 模型中固定的前向或后向因式分解顺序，而是最大化所有可能因式分解顺序的期望对数似然。由于对因式分解顺序的排列操作，每个位置的语境都包含来自左侧和右侧的 token。因此，每个位置都能学习来自所有位置的语境信息，即捕捉双向语境。

其次，作为一个泛化 AR 语言模型，XLNet 不依赖残缺数据。因此，XLNet 不会有 BERT 的预训练-微调差异。同时，自回归目标提供一种自然的方式，来利用乘法法则对预测 token 的联合概率执行因式分解（factorize），这消除了 BERT 中的独立性假设。除了提出一个新的预训练目标，XLNet 还改进了预训练的架构设计。

受到 AR 语言建模领域最新进展的启发，XLNet 将 Transformer-XL 的分割循环机制（segment recurrence mechanism）和相对编码范式（relative encoding）整合到预训练中，实验表明，这种做法提高了性能，尤其是在那些包含较长文本序列的任务中。
